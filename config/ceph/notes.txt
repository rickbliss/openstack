#sudo su

CEPH_RELEASE=18.2.7 # replace this with the active release
curl --silent --remote-name --location https://download.ceph.com/rpm-${CEPH_RELEASE}/el9/noarch/cephadm

#Noble doesnt ahve release so we add jammmy
sudo apt-add-repository 'deb https://download.ceph.com/debian-reef/ jammy main'

# ceph osd crush rule ls 
# ceph osd crush rule dump ssd_rule

sudo ceph crash archive-all

#Since OSs have lots of random I/O and bootstorms lets just use ssd for all


sudo ceph osd pool create ssd-vms 128 128 replicated ssd_rule
sudo ceph osd pool set ssd-vms size 3
sudo ceph osd pool set ssd-vms min_size 2

sudo ceph osd pool create ssd-volumes 128 128 replicated ssd_rule
sudo ceph osd pool set ssd-volumes size 3
sudo ceph osd pool set ssd-volumes min_size 2

sudo ceph osd pool create hdd-vms 128 128 replicated hdd_rule
sudo ceph osd pool set hdd-vms size 3
sudo ceph osd pool set hdd-vms min_size 2

sudo ceph osd pool create hdd-volumes 128 128 replicated hdd_rule
sudo ceph osd pool set hdd-volumes size 3
sudo ceph osd pool set hdd-volumes min_size 2

sudo ceph osd pool create backups 128 128 replicated hdd_rule
sudo ceph osd pool set backups size 3
sudo ceph osd pool set backups min_size 2

sudo ceph osd pool create images 128 128 replicated ssd_rule
sudo ceph osd pool set images size 3
sudo ceph osd pool set images min_size 2

	

ceph osd pool application enable ssd-vms rbd
ceph osd pool application enable ssd-volumes rbd
ceph osd pool application enable hdd-vms rbd
ceph osd pool application enable hdd-volumes rbd
ceph osd pool application enable backups rbd
ceph osd pool application enable images rbd




# https://docs.ceph.com/en/reef/rbd/rbd-openstack/ and https://febryandana.xyz/posts/deploy-ceph-openstack-cluster/
#!/usr/bin/env bash
set -euo pipefail

echo "ðŸ”¹ Setting caps for client.glance..."
ceph auth caps client.glance \
  mon 'profile rbd' \
  osd 'profile rbd pool=images' \
  mgr 'profile rbd pool=images' 



echo "ðŸ”¹ Setting caps for client.cinder..." 
ceph auth caps client.cinder \
  mon 'profile rbd' \
  osd 'profile rbd pool=ssd-volumes, profile rbd pool=hdd-volumes, profile rbd pool=ssd-vms, profile rbd pool=hdd-vms,profile rbd-read-only pool=images' \
  mgr 'profile rbd pool=ssd-volumes, profile rbd pool=hdd-volumes, profile rbd pool=hdd-vms, profile rbd pool=ssd-vms'


echo "ðŸ”¹ Setting caps for client.nova..."
ceph auth caps client.nova \
  mon 'profile rbd' \
  osd 'allow class-read object_prefix rbd_children' \
  osd 'profile rbd pool=images' \
  mgr 'profile rbd-read-only pool=images'
  mgr 'profile rbd pool=hdd-volumes, profile rbd pool=ssd-volumes, mgr 'profile rbd pool=ssd-vms, profile rbd pool=hdd-vms''

echo "ðŸ”¹ Setting caps for client.cinder-backup..."
ceph auth caps client.cinder-backup \
  mon 'allow r' \
  osd 'allow class-read object_prefix rbd_children' \
  osd 'allow rwx pool=backups' \
  osd 'allow rwx pool=hdd-volumes' \
  osd 'allow rwx pool=ssd-volumes'

echo "âœ… All caps applied."

echo "ðŸ”¹ Exporting keyring for client.glance..."
ceph auth get client.glance -o /etc/ceph/ceph.client.glance.keyring

echo "ðŸ”¹ Exporting keyring for client.cinder..."
ceph auth get client.cinder -o /etc/ceph/ceph.client.cinder.keyring

echo "ðŸ”¹ Exporting keyring for client.nova..."
ceph auth get client.nova -o /etc/ceph/ceph.client.nova.keyring

echo "ðŸ”¹ Exporting keyring for client.cinder-backup..."
ceph auth get client.cinder-backup -o /etc/ceph/ceph.client.cinder-backup.keyring

echo "âœ… All keyrings exported successfully."


#Not sure if kolla copies all the keyrings so lets put them in koll and on the host etc?

#I think kolla does this be merging all of the different serice confs into one via globals etc


for host in localhost bc00 bc01 bc02 bc03 bc04; do
  sudo ceph auth get-or-create client.glance | ssh "$host" sudo tee /etc/ceph/ceph.client.glance.keyring
  sudo ceph auth get-or-create client.cinder | ssh "$host" sudo tee /etc/ceph/ceph.client.cinder.keyring
  sudo ceph auth get-or-create client.nova   | ssh "$host" sudo tee /etc/ceph/ceph.client.nova.keyring
  sudo ceph auth get-or-create client.cinder-backup   | ssh "$host" sudo tee /etc/ceph/ceph.client.cinder-backup.keyring
  sudo scp -i ~/.ssh/id_rsa /etc/ceph/ceph.conf $host:/etc/ceph
done

cp /etc/ceph/ceph.client.glance.keyring /etc/kolla/config/glance/
cp /etc/ceph/ceph.client.cinder.keyring /etc/kolla/config/cinder/
cp /etc/ceph/ceph.client.cinder.keyring /etc/kolla/config/cinder-backup/
cp /etc/ceph/ceph.client.cinder.keyring /etc/kolla/config/cinder-volume/


sudo tee ceph.conf <<EOF
[client]
rbd cache = true
rbd cache writethrough until flush = true

[global]
fsid = 4c1c61ae-397e-11f0-b26f-0cc47a7cbc82
mon_host = [v2:10.0.51.40:3300/0,v1:10.0.51.40:6789/0] [v2:10.0.51.34:3300/0,v1:10.0.51.34:6789/0] [v2:10.0.51.37:3300/0,v1:10.0.51.37:6789/0] [v2:10.0.51.38:3300/0,v1:10.0.51.38:6789/0]
  
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx

[client.glance]
rbd default data pool = images
keyring = /etc/ceph/ceph.client.glance.keyring

[client.cinder]
rbd default data pool = hdd-volumes
keyring = /etc/ceph/ceph.client.cinder.keyring


[client.cinder-backup]
rbd default data pool = backups
keyring = /etc/ceph/ceph.client.cinder-backup.keyring

[client.nova]
rbd default data pool = ssd-vms
keyring = /etc/ceph/ceph.client.nova.keyring
EOF

for host in bc00 bc01 bc02 bc03 bc04; do
  echo "Copying ceph.conf to $host..."
  ssh $host 'mkdir -p /tmp/ceph/'
  scp ceph.conf "$host:/tmp/ceph/"
  ssh "$host" "sudo mv /tmp/ceph/* /etc/ceph/&& sudo chown root:root /etc/ceph/*"
done

###
# Do Has Cluster?
# 
##


for img in $(openstack image list -f value -c ID); do
  echo "Updating properties on image $img..."
  openstack image set \
    --property hw_scsi_model=virtio-scsi \
    --property hw_disk_bus=scsi \
    --property hw_qemu_guest_agent=yes \
    --property os_require_quiesce=yes \
    "$img"
done
